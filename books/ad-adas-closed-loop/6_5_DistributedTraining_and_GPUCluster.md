---
title: "6.5 分散学習の効率化と GPU Cluster 設計"
---

# 6.5 分散学習の効率化と GPU Cluster 設計

この節では、分散学習 (distributed training) の効率化と GPU クラスタ設計について解説します。Data Parallel / Model Parallel / Pipeline Parallel、GPU 故障・ジョブ再開・チェックポイント設計、スケジューラ・ジョブキュー・優先度制御などを整理し、データ中心・Closed-Loop の観点から「再学習サイクルを短く保つためのインフラ設計」を考えます。

## 分散学習の基本パラダイム

自動運転モデルは、マルチカメラ・マルチモーダル入力と大容量の BEV 特徴マップを扱うため、単一 GPU ではメモリ・計算量が足りないことが多いです。そのため、分散学習のパラダイムを組み合わせて大規模モデルを効率的に学習します。

- Data Parallel: ミニバッチを複数 GPU に分割し、それぞれが同じモデルを持って勾配を計算し、All-Reduce で勾配を集約する方式です。PyTorch DDP などが代表的です。
- Model Parallel: モデル自体をレイヤ単位・テンソル単位で複数 GPU に分割し、順伝播・逆伝播を分散して計算します。巨大な Transformer などで用いられます。
- Pipeline Parallel: モデルをステージに分割し、ミニバッチをマイクロバッチに分割してパイプライン的に流す方式です。モデルサイズとスループットの両方を拡張できます。

実務では、ZeRO Optimizer を用いたメモリ分割や、テンソル並列・パイプライン並列を組み合わせたハイブリッド構成などが利用されます。AD 向けモデルでは、画像・点群・時系列処理が混在するため、どの部分をどの並列方式で処理するかの設計が重要になります。

## GPU クラスタアーキテクチャ

GPU クラスタは、通常以下のような構成要素から成ります。

- GPU ノード: 複数 GPU（例: 8x GPU）を搭載したサーバ。高速な NVLink や PCIe によって GPU 間が接続されています。
- ストレージ: 高スループットな分散ファイルシステムやオブジェクトストレージ。トレーニングデータやチェックポイントを格納します。
- ネットワーク: ノード間を結ぶ高速ネットワーク（InfiniBand / 100GbE など）。分散学習の All-Reduce 通信の性能に直結します。
- スケジューラ: Kubernetes や Slurm、社内製ジョブスケジューラなど。ジョブのキューイング・リソース割り当て・優先度管理を行います。

Closed-Loop の観点では、「インシデント発生から再学習完了までのリードタイム」を短くすることが重要です。そのために、GPU クラスタの設計段階から「短時間で終わる実験ジョブ」と「長時間走る大規模学習ジョブ」を適切に混在させられるようなキュー設計や、プリアロケーション戦略を検討する必要があります。

## チェックポイント設計と障害耐性

長時間にわたる分散学習では、GPU の一時的なエラーやノード障害が避けられません。これに備えるために、チェックポイント (checkpoint) の設計と障害耐性 (fault tolerance) を組み込みます。

- チェックポイント頻度: 数時間に一度、あるいは epoch ごとにモデルパラメータ・オプティマイザ状態・学習進捗（データオフセット）を保存します。
- ライトパターン: すべての GPU が同時にストレージに書き込むと I/O が輻輳するため、rank 0 のプロセスのみが書き込む、あるいは分散チェックポイントを取るなどの工夫をします。
- 再開ロジック: ジョブが途中で失敗した場合、最新のチェックポイントから再開できるように、学習スクリプトに再開用フラグやデータローダの状態復元ロジックを実装します。

障害耐性は Closed-Loop の再学習サイクルの信頼性に直結します。再学習ジョブが頻繁に失敗するような環境では、インシデント対応のリードタイムが読めず、組織的な意思決定にも悪影響を与えます。

## スケジューラ・ジョブキュー・優先度制御

GPU クラスタを複数チーム・複数プロジェクトで共有する場合、ジョブスケジューリングのルール設計が重要になります。

- キュークラスの設計: バッチ学習用キュー、短時間インタラクティブキュー、実験用キューなどを分け、それぞれに最大占有 GPU 数や優先度を設定します。
- 優先度制御: 安全上重要なインシデント対応ジョブやリリース前の検証ジョブには高い優先度を付与し、即時に GPU を確保できるようにします。
- フェアシェア: チームごと・プロジェクトごとに GPU クォータを設定し、特定チームがリソースを独占しないようにします。

これらのルールを DataOps / MLOps チームと合意し、実験トラッキングやリソース使用ログと組み合わせて可視化することで、「どの Closed-Loop サイクルにどれだけの計算資源を投入したか」を後から分析できます。

## 分散データローディングとネットワーク設計

分散学習では、DataLoader の設計とネットワークトポロジも性能に大きく影響します。

- シャーディング戦略: 各 GPU プロセスが異なる shard を読み込むようにし、データの重複を避けます。
- プリフェッチとバッファリング: I/O の遅延を隠蔽するために、複数バッチ先読みとリングバッファを活用します。
- ローカルキャッシュ: オブジェクトストレージからの読み込みをローカル SSD にキャッシュし、ネットワークトラフィックを削減します。

ネットワーク設計では、All-Reduce 通信がボトルネックにならないよう、GPU 同士が高速リンクで接続されたトポロジ（fat-tree、dragonfly など）を採用することが一般的です。

## Closed-Loop における GPU Cluster の役割

Closed-Loop 開発サイクルでは、データ収集・ラベリング・シミュレーションと並んで、GPU クラスタが「改善サイクルのスループット」を決定する重要な要素になります。

- 再学習 SLA: インシデント発生から「データ選択・ラベリング・再学習・評価」が完了するまでのターゲット時間を定め、その中で GPU クラスタがどの程度の時間を占めてよいかを逆算します。
- バッチ vs ストリーミング: 定期的なバッチ再学習に加え、オンラインでの微調整や continual learning を行うかどうかを検討します。
- コスト管理: GPU 使用量とクラウドコストをモニタリングし、優先度の低い実験を抑制したり、効率的な学習戦略（第 6.4 節）を採用したりすることで、限られたリソースを有効活用します。

このように、分散学習と GPU クラスタ設計は、単に学習時間を短縮するだけでなく、Closed-Loop の全体設計における「改善サイクルの速度とコスト」を規定する重要な要素となります。
